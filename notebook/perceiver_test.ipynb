{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import math\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "NUM_TRAIN = 49000\n",
    "\n",
    "# The torchvision.transforms package provides tools for preprocessing data\n",
    "# and for performing data augmentation; here we set up a transform to\n",
    "# preprocess the data by subtracting the mean RGB value and dividing by the\n",
    "# standard deviation of each RGB value; we've hardcoded the mean and std.\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            ])\n",
    "\n",
    "# We set up a Dataset object for each split (train / val / test); Datasets load\n",
    "# training examples one at a time, so we wrap each Dataset in a DataLoader which\n",
    "# iterates through the Dataset and forms minibatches. We divide the CIFAR-10\n",
    "# training set into train and val sets by passing a Sampler object to the\n",
    "# DataLoader telling how it should sample from the underlying Dataset.\n",
    "cifar10_train = dset.CIFAR10('../data/CIFAR10', train=True, download=True,\n",
    "                             transform=transform)\n",
    "loader_train = DataLoader(cifar10_train, batch_size=64, \n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "cifar10_val = dset.CIFAR10('../data/CIFAR10', train=True, download=True,\n",
    "                           transform=transform)\n",
    "loader_val = DataLoader(cifar10_val, batch_size=64, \n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "\n",
    "cifar10_test = dset.CIFAR10('../data/CIFAR10', train=False, download=True, \n",
    "                            transform=transform)\n",
    "loader_test = DataLoader(cifar10_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "dtype = torch.float32 # We will be using float throughout this tutorial.\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss.\n",
    "print_every = 100\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, epochs=1):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                check_accuracy(loader_val, model)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier_encode(x, max_freq, num_bands = 4, base = 2):\n",
    "    x = x.unsqueeze(-1)\n",
    "    device, dtype, orig_x = x.device, x.dtype, x\n",
    "\n",
    "    scales = torch.logspace(0., math.log(max_freq / 2) / math.log(base), num_bands, base = base, device = device, dtype = dtype)\n",
    "    scales = scales[(*((None,) * (len(x.shape) - 1)), Ellipsis)]\n",
    "\n",
    "    x = x * scales * math.pi\n",
    "    x = torch.cat([x.sin(), x.cos()], dim=-1)\n",
    "    x = torch.cat((x, orig_x), dim = -1)\n",
    "    return x\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, query_dim, dim_head, num_heads=1, context_dim = None, dropout=0):\n",
    "        super().__init__()\n",
    "        embed_dim = dim_head * num_heads\n",
    "        if context_dim is None:\n",
    "            context_dim = query_dim\n",
    "\n",
    "        self.H = num_heads\n",
    "        self.E = embed_dim\n",
    "        self.q_linear = nn.Linear(query_dim, embed_dim)\n",
    "        self.k_linear = nn.Linear(context_dim, embed_dim)\n",
    "        self.v_linear = nn.Linear(context_dim, embed_dim)\n",
    "        self.o_linear = nn.Linear(embed_dim, query_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, context = None, attn_mask=None):\n",
    "        if context is None:\n",
    "            context = query\n",
    "        N, S, query_dim = query.shape\n",
    "        N, T, context_dim = context.shape\n",
    "\n",
    "        q = self.q_linear(query)\n",
    "        k = self.k_linear(context)\n",
    "        v = self.v_linear(context)\n",
    "        q_split = q.view((N, S, self.H, int(self.E/self.H))).transpose(1, 2)\n",
    "        k_split = k.view((N, T, self.H, int(self.E/self.H))).transpose(1, 2)\n",
    "        v_split = v.view((N, T, self.H, int(self.E/self.H))).transpose(1, 2)\n",
    "        a = torch.matmul(q_split, k_split.transpose(2, 3))/math.sqrt(self.E/self.H)\n",
    "        if attn_mask is not None:\n",
    "            a = a.masked_fill(~(attn_mask.type(torch.bool)), -math.inf)\n",
    "        e = torch.nn.functional.softmax(a, dim=3)\n",
    "        y = torch.matmul(e, v_split)\n",
    "        output = self.o_linear(y.transpose(1, 2).reshape(N, S, self.E))\n",
    "        return self.dropout(output)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mult = 4, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim * mult, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionPerceiver(nn.Module):\n",
    "    def __init__(self, input_dim, input_channels=3,\n",
    "                 max_freq = 8, num_freq_bands = 4,\n",
    "                 num_iterations = 1, num_transformer_blocks = 4,\n",
    "                 num_latents = 32, latent_dim = 128,\n",
    "                 cross_heads = 1, cross_dim_head = 8,\n",
    "                 latent_heads = 2, latent_dim_head = 8,\n",
    "                 num_classes = 10,\n",
    "                 attn_dropout = 0., ff_dropout = 0.):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_latents = num_latents\n",
    "        self.latent_dim = latent_dim\n",
    "        self.max_freq = max_freq\n",
    "        self.num_freq_bands = num_freq_bands\n",
    "        input_channels *= 9\n",
    "        # perceiver stacks\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for i in range(num_iterations): # build each perceiver cell\n",
    "            cell = nn.ModuleList([])\n",
    "            # cross attention module\n",
    "            cell.append(nn.LayerNorm(latent_dim))\n",
    "            cell.append(nn.LayerNorm(input_dim * input_channels))\n",
    "            cell.append(MultiHeadAttention(latent_dim, dim_head = cross_dim_head,\n",
    "                                           num_heads = cross_heads, context_dim = input_channels,\n",
    "                                           dropout = attn_dropout))\n",
    "            # feed forward\n",
    "            cell.append(nn.LayerNorm(latent_dim))\n",
    "            cell.append(FeedForward(latent_dim, dropout = ff_dropout))\n",
    "\n",
    "            # latent transformer\n",
    "            latent_transformer = nn.ModuleList([])\n",
    "            for j in range(num_transformer_blocks):\n",
    "                latent_transformer_block = nn.ModuleList([])\n",
    "                # self attention\n",
    "                latent_transformer_block.append(nn.LayerNorm(latent_dim))\n",
    "                latent_transformer_block.append(MultiHeadAttention(latent_dim, dim_head = latent_dim_head,\n",
    "                                           num_heads = latent_heads, dropout = attn_dropout))\n",
    "                # feed forward\n",
    "                latent_transformer_block.append(nn.LayerNorm(latent_dim))\n",
    "                latent_transformer_block.append(FeedForward(latent_dim, dropout = ff_dropout))\n",
    "                latent_transformer.append(latent_transformer_block)\n",
    "            cell.append(latent_transformer)\n",
    "\n",
    "            self.layers.append(cell)\n",
    "\n",
    "        self.to_logits = nn.Sequential(\n",
    "                            nn.LayerNorm(latent_dim),\n",
    "                            nn.Linear(latent_dim, num_classes))\n",
    "\n",
    "    def forward(self, data, attn_mask = None, latent_init = None, seed = None):\n",
    "        # flatten\n",
    "        N, C, H, W = data.shape\n",
    "        data = data.view(N, C, -1).transpose(1, 2)\n",
    "\n",
    "        # encoding\n",
    "        data = fourier_encode(data, self.max_freq, self.num_freq_bands)\n",
    "\n",
    "        # determine the initial latent vector\n",
    "        if latent_init is None:\n",
    "            if seed is not None:\n",
    "                torch.manual_seed(seed)\n",
    "            latent_init = torch.randn(self.num_latents, self.latent_dim).unsqueeze(0).repeat([N, 1, 1])\n",
    "            latent_init.requires_grad = False\n",
    "        else:\n",
    "            assert latent_init.shape == (num_latents, latent_dim)\n",
    "            latent_init.unsqueeze(0).repeat([N, 1, 1])\n",
    "\n",
    "        self.latent_init = nn.Parameter(latent_init)\n",
    "\n",
    "        x = latent_init\n",
    "        for cell in self.layers:\n",
    "            # cross attention\n",
    "            y = cell[1](data.reshape(N, -1))\n",
    "            x = cell[2](cell[0](x), y.reshape(N, H*W, -1), attn_mask=attn_mask) + x\n",
    "            # feed forward\n",
    "            x = cell[4](cell[3](x)) + x\n",
    "            # latent transformer\n",
    "            for latent_transformer in cell[5]:\n",
    "                # self attention\n",
    "                x = latent_transformer[1](latent_transformer[0](x), attn_mask=attn_mask) + x\n",
    "                # feed forward\n",
    "                x = latent_transformer[3](latent_transformer[2](x)) + x\n",
    "\n",
    "        x = x.mean(dim = -2)\n",
    "        return self.to_logits(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisionPerceiver(1024, num_transformer_blocks=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 2.3777\n",
      "Checking accuracy on validation set\n",
      "Got 103 / 1000 correct (10.30)\n",
      "\n",
      "Iteration 100, loss = 2.2177\n",
      "Checking accuracy on validation set\n",
      "Got 181 / 1000 correct (18.10)\n",
      "\n",
      "Iteration 200, loss = 2.1532\n",
      "Checking accuracy on validation set\n",
      "Got 240 / 1000 correct (24.00)\n",
      "\n",
      "Iteration 300, loss = 1.9445\n",
      "Checking accuracy on validation set\n",
      "Got 261 / 1000 correct (26.10)\n",
      "\n",
      "Iteration 400, loss = 1.9280\n",
      "Checking accuracy on validation set\n",
      "Got 263 / 1000 correct (26.30)\n",
      "\n",
      "Iteration 500, loss = 1.9815\n",
      "Checking accuracy on validation set\n",
      "Got 251 / 1000 correct (25.10)\n",
      "\n",
      "Iteration 600, loss = 1.9697\n",
      "Checking accuracy on validation set\n",
      "Got 261 / 1000 correct (26.10)\n",
      "\n",
      "Iteration 700, loss = 1.8497\n",
      "Checking accuracy on validation set\n",
      "Got 261 / 1000 correct (26.10)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(model, optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
