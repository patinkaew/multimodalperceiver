{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb6db809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torch.optim as optim\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d517749e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.dirname(sys.path[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49c78f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.encoder_perceiver import EncoderPerceiver\n",
    "from model.encoder_cnn import EncoderResNet\n",
    "from model.decoder import CaptioningTransformer\n",
    "from utils.flickr8k_util import FlickrDataset, CapsCollate\n",
    "from model.utils import show_image\n",
    "from experiment.solver import CaptioningSolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "856f2783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8318fbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "transform = T.Compose([\n",
    "                T.Resize((224, 224)),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) # for flickr8k\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6adb4b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/Users/pinkaew/Stanford Files/Spring 2021/CS 231N/project/multimodalperceiver\"\n",
    "data_location = base_dir + \"/data/Flickr8k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4a8f3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building vocab\n",
      "buidling caption alias\n"
     ]
    }
   ],
   "source": [
    "dataset =  FlickrDataset(\n",
    "    root_dir = data_location + \"/Images\",\n",
    "    caption_file = data_location + \"/captions.txt\",\n",
    "    transform = transform,\n",
    "    verbose = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa0030e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_per_image = 5\n",
    "num_data = len(dataset)\n",
    "num_figures = num_data/caption_per_image\n",
    "test_split = 0.2\n",
    "val_split = 0.2\n",
    "train_idx = int((1 - test_split - val_split) * num_figures)\n",
    "train_abs_idx = train_idx * caption_per_image\n",
    "val_idx = train_idx + int(val_split * num_figures)\n",
    "val_abs_idx = val_idx * caption_per_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a585aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = range(train_abs_idx)\n",
    "val_indices = range(train_abs_idx, val_abs_idx)\n",
    "test_indices = range(val_abs_idx, num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cba2874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing the dataloader\n",
    "# setting the constants\n",
    "BATCH_SIZE = 5\n",
    "NUM_WORKER = 4\n",
    "\n",
    "# token to represent the padding\n",
    "pad_idx = dataset.vocab.stoi[\"<NULL>\"]\n",
    "\n",
    "loader_train = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKER,\n",
    "    sampler = sampler.SubsetRandomSampler(range(10)),\n",
    "    collate_fn=CapsCollate(pad_idx=pad_idx,batch_first=True)\n",
    ")\n",
    "\n",
    "loader_val = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKER,\n",
    "    sampler = sampler.SubsetRandomSampler(val_indices),\n",
    "    collate_fn=CapsCollate(pad_idx=pad_idx,batch_first=True)\n",
    ")\n",
    "\n",
    "loader_test = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKER,\n",
    "    sampler = sampler.SubsetRandomSampler(test_indices),\n",
    "    collate_fn=CapsCollate(pad_idx=pad_idx,batch_first=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eca9881e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create models\n",
    "input_dim = 224 * 224\n",
    "input_channels = 3\n",
    "feature_dim = 128\n",
    "word_emb_dim = 256\n",
    "encoder = EncoderPerceiver(input_dim, input_channels = input_channels, \n",
    "                             num_iterations = 2, num_transformer_blocks = 4,\n",
    "                             num_latents=32, latent_dim = feature_dim, \n",
    "                             cross_heads=4, cross_dim_head=16, \n",
    "                             latent_heads=4, latent_dim_head=16,\n",
    "                             attn_dropout=0.5, ff_dropout=0.5)\n",
    "decoder = CaptioningTransformer(dataset.vocab.stoi, input_dim=feature_dim, \n",
    "                                  wordvec_dim = word_emb_dim, max_length=30)\n",
    "\n",
    "#encoder_r = EncoderResNet(feature_dim)\n",
    "#decoder_r = CaptioningTransformer(dataset.vocab.stoi, input_dim=feature_dim, \n",
    "#                                  wordvec_dim = word_emb_dim, max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1909c9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_optimizer = optim.Adam(encoder.parameters())\n",
    "decoder_optimizer = optim.Adam(decoder.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2390feb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = CaptioningSolver(encoder, decoder, \n",
    "                          encoder_optimizer, decoder_optimizer,\n",
    "                          dataset.vocab.itos, dataset.caption_alias, \n",
    "                          device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "935dff02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#solver.train(loader_train, loader_val, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c0aa5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "applying transformation\n",
      "applying transformation\n"
     ]
    }
   ],
   "source": [
    "for images, captions in loader_train:\n",
    "    show_image(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775acfd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
